<!DOCTYPE html>
<html lang="en">

<head>
	<title>Audio Reactive Ray Marching GPU</title>

	<!-- seo -->
	<meta name="author" content="Michael Crum, Antti Meriluoto">
	<meta name="description" content="FPGA implementation of a audio reactive ray marching GPU">
	<meta name="keywords" content="portfolio,developer,robotics,personal">

	<!-- display -->
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width">

	<!-- icon -->
	<link rel="icon" type="image/png" sizes="32x32" href="../global_assets/icons/favicon-32x32.png" />
	<link rel="icon" type="image/png" sizes="16x16" href="../global_assets/icons/favicon-16x16.png" />

	<!-- stylesheets -->
	<link rel="stylesheet" href="./styles/index.css">
	<link rel="stylesheet" href="./styles/highlight/styles/base16/bright.min.css">
	<link rel="stylesheet" href="./styles/katex/katex.min.css">

	<!-- syntax highlighting-->
	<script src="../styles/highlight/highlight.min.js"></script>
	<script src="../styles/highlight/languages/verilog.min.js"></script>
	<script>
		hljs.highlightAll();
	</script>

	<!-- latex support-->
	<script defer src="../styles/katex/katex.min.js"></script>

	<!-- font -->
	<link href="https://fonts.googleapis.com/css2?family=Pacifico&display=swap" rel="stylesheet" />

	<!-- iFrame optimization -->
	<script>
		checkVisibility();
		document.addEventListener("scroll", (event) => {
			checkVisibility();
		});

		function checkVisibility() {
			const frames = document.getElementsByTagName("iframe");
			for (var i = 0; i < frames.length; i++) {
				frame = frames[i];
				if (isInViewport(frame)) {
					frame.style.visibility = "visible";
				} else {
					frame.style.visibility = "hidden";
				}
			}
		}

		function isInViewport(elm) {
			var rect = elm.getBoundingClientRect();
			var viewHeight = Math.max(document.documentElement.clientHeight, window.innerHeight);
			return !(rect.bottom < 0 || rect.top - viewHeight >= 0);
		}
	</script>
</head>

<body>
	<article id="article">
		<h1>Audio Reactive Visualizations Using Machine Learning and a Custom FPGA Raymarching GPU</h1>
<p><em>By Michael Crum (mmc323@cornell.edu) and Antti Meriluoto (ahm234@cornell.edu)</em></p>
<p><img src="./assets/banner.webp" alt="Banner"></p>
<h2>Introduction</h2>
<p>In a tradition as old as time, sights and sounds are often paired together to induce emotion in the observer.
Incredible reactions can be created with careful pairings of the two, although the exact relation between image and audio is, as with many things distinctly human, difficult to pin down precisely.
Of course, this difficulty only seems to make the challenge more appealing to creatives, scientists, and all varieties of people in between.</p>
<p>For our final project, we chose to attempt a unique approach to the problem, utilizing the skills and tools we learned during the class to create visual interpretations of songs.
We used machine learning to extract emotional meaning from the music, which in turn impacted the ray marched visuals generated on the FPGA.
In this write-up, we will summarize the various techniques used to realize our final product.</p>
<h2>Results</h2>
<p>Although it’s somewhat unconventional to show results at the beginning of the write-up, seeing the final product might help you visualize what we’re going on about in the following sections.</p>
<p><em>I’ll insert pictures/videos here in the final website</em></p>
<h2>Sentiment Analysis</h2>
<p><em>section in progress</em></p>
<h2>FPGA-Based Raymarching GPU</h2>
<h3>The ray marching algorithm</h3>
<p>For brevity, this section will offer an abridged description of the raymarching algorithm.
If you wish to learn more about the algorithm, Michael <a href="https://michael-crum.com/raymarching/">has written a full article about it on his website</a> (with pretty demos too!).</p>
<p>Raymarching is a form of <a href="https://en.wikipedia.org/wiki/Physically_based_rendering">physically based rendering</a>: rendering techniques that model the interaction of lights and physical objects to emulate real-world vision.
Because emulating billions of photons emitted from light sources is intractable, physically based rendering techniques instead work backward to trace the path of photons that ultimately end up hitting the camera (or eye, depending on how you prefer to think of it).</p>
<p>At a base level, the goal of rendering is to assign a color to each screen pixel.
Think of your screen as a window.
Through the window is the scene we wish to render, made up of simple shapes.
The screen/window can be quantized by its pixels, each specifying a unique coordinate on the window.
For light from the scene to enter your eye, it must pass through one of these pixels in the window.
Suppose we calculate the vector from your eye through each pixel on the window.
We can then use this vector to work backward and determine information about the photons that would enter through that pixel.
From this information, we can determine the color of that pixel.
Figure X visualizes this process for one pixel.</p>
<p><img src="./assets/image_plane.webp" alt="alt_text"></p>
<p><em>Figure X: From pixels to rays (Credit: <a href="https://michaelwalczyk.com/blog-ray-marching.html">Michael Walczyk</a>)</em></p>
<p>The process is well-researched in computer graphics and is computed using the inverse <a href="https://en.wikipedia.org/wiki/Camera_matrix">camera projection matrix</a>.
For our virtual camera, this operation boils down to just a couple of operations:</p>
<pre><code>vec2 xy = coordinate.xy - resolution.xy / 2.0;
float z = resolution.y / tan(radians(field_of_view) / 2.0);
vec3 view_direction = normalize(vec3(xy, -z));
</code></pre>
<p>With per-pixel rays calculated, we can begin tracing them backward into the scene.
The first challenge is determining where a ray intersects the scene.
One approach is to analytically compute the ray-scene intersection.
This is the basis of ray tracing, but this computation is expensive and scales poorly with the scene's complexity.
For use on the resource-constrained FPGA, we need to be a bit more clever.</p>
<p>Enter ray marching, an iterative solution for computing ray-scene intersections.
The key abstraction of the ray marching algorithm is the signed distance field (SDF).
An SDF is a function that takes in a point in space and returns the distance from the point to a scene.
To illustrate the concept I will use 2D SDFs, but they trivially extend to 3D space.
The simplest 2D SDF is a circle located at the origin.
The calculation is trivial:</p>
<pre><code>
return length(point) - radius;

</code></pre>
<p>Such distance functions exist for all manner of primitives, both 2D and 3D (see Figure X).</p>
<p><img src="./assets/2d_sdf.webp" alt="alt_text"></p>
<p><em>Figure X: Raymarched primitives</em></p>
<p>Ray marching leverages SDFs to know how far it can <strong>safely</strong> step along a ray without intersecting with the scene.
The process is as follows:</p>
<ol>
<li>Set point <eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span></span></span></span></eq> at the camera origin.</li>
<li>Evaluate SDF to find minimum distance <eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span></span></span></span></eq> from the scene.
<ol>
<li>If <eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo>&lt;</mo><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">d &lt; \epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335em;vertical-align:-0.0391em;"></span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">ϵ</span></span></span></span></eq> (some small number), you’ve hit the scene</li>
<li>Else continue</li>
</ol>
</li>
<li>Step <eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span></span></span></span></eq> units along the ray, ie <eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>+</mo><mo>=</mo><mi>r</mi><mi>a</mi><mi>y</mi><mo>∗</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">p += ray * d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span><span class="mord">+</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6597em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span></span></span></span></eq></li>
<li>Goto 1</li>
</ol>
<p>It's important to notice that step 3 is <strong>guaranteed</strong> to not set <eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span></span></span></span></eq> inside of an object.
Because <eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span></span></span></span></eq> starts in free space, and <eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span></span></span></span></eq> is the minimum distance to the scene along <em>any</em> direction, stepping along _any ray by <eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span></span></span></span></eq> will cause the new distance to be &gt;= 0.
If the ray does intersect with the scene, <eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span></span></span></span></eq> will converge to zero, and the algorithm will register the point as an intersection.
Figure X illustrates the algorithm on a 2D scene.
Each blue point is <eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span></span></span></span></eq> after some number of iterations, and each circle represents <eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span></span></span></span></eq> evaluated at <eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span></span></span></span></eq> for each iteration.</p>
<iframe src="https://michael-crum.com/ThreeJS-Raymarcher/2d_demo.html" title="2D Demo" style="visibility: visible;"></iframe>
<p><em>Figure X: 2D ray marching demo</em></p>
<p>SDFs of multiple objects can be combined simply by taking the minimum of their individual SDFs.
Similar operations exist for intersection and difference.
SDFs can also be deformed to scale, rotate, twist, and repeat the SDF throughout space.
I won’t go over all of the operations here, but I’ll leave<a href="https://iquilezles.org/articles/distfunctions/"> this excellent reference on the topic</a>.</p>
<p>Ray marching outperforms ray tracing in situations where SDFs are more efficient to compute than analytic intersections.
Being an iterative solution, it is also well suited to be broken up into multiple clock cycles on the FPGA.
Notice also that each pixel is computed completely independently, perfect for massive parallelization.
This is the exact purpose of a GPU and the reason they are so valuable for graphical applications.</p>
<h3>Floating point and vector math on an FPGA</h3>
<p>To have any hope at running the ray marching algorithm, we need a fractional representation that will run on the FPGA.
The traditional solution to this problem is using a fixed-point representation, but this comes with trade-offs either in magnitude or precision.
Because ray marching operates over a wide range of magnitudes, fixed-point was off the table.
We instead decided to use the <a href="https://people.ece.cornell.edu/land/courses/ece5760/DE1_SOC/HPS_peripherials/Floating_Point_index.html">1.8.18 floating-point implementation</a> written by past students and improved by Bruce Land.
This gives an impressive range both for low-magnitude precision, important for normalized vectors, and high-magnitude representation, important for rendering objects at long range.</p>
<p>To simplify many repeated operations in our Verilog, we wrote a vector math library utilizing floating point math.
The library can be found in vector_ops.v in the appendix and includes dot products, scalar multiplication, addition, and 3x3 matrix multiplication.</p>
<h3>Prototyping and verification</h3>
<p>Before launching into a full Verilog implementation, we created a reference implementation in GLSL (OpenGL Shading Language).
GLSL is a C-like language specifically for writing shaders (programs that run per pixel on the GPU).
The GLSL implementation is only 100 lines and can be found in the appendix under fractal frag.
We used this reference implementation to render the Serpinski pyramid fractal (Figure X).</p>
<p><img src="./assets/serpinski.webp" alt="Serpinski Pyramid"></p>
<p><em>Figure X: Serpinski pyramid rendered with GLSL</em></p>
<p>After verifying the GLSL design, we moved on to Verilog.
Because compiling code for the FPGA takes many minutes, we looked for a simulation tool that could show VGA output without lengthy Quartus compile times.
We found the tool <a href="https://verilator.org/guide/latest/">Verilator</a>, which compiles Verilator source code into multithreaded C++ object files.
The Verilated source code can then be linked against and the output of the model can be passed into any C++ functions of our choosing.
We used <a href="https://github.com/libsdl-org/SDL">SDL</a> to render the output VGA of our model directly to a screen buffer, which can then be rendered into a window.
This <a href="https://projectf.io/posts/verilog-sim-verilator-sdl/">webpage from Project F</a> was a great resource for setting up the system.
Simulating this way allowed us to render 2-3 frames per second, far from real-time but many times faster than a full Quartus compile.
Using this strategy we were able to quickly iterate on the design and fix bugs many times more efficiently than in previous labs.
See Figure X for an example of Verilated VGA output.</p>
<p><img src="./assets/verilated_vga.webp" alt="Verilated VGA output"></p>
<p><em>Figure X: Output of Verilated model rendered with SDL</em></p>
<h3>Architecture</h3>
<p>Our ray-marching GPU uses a pipeline architecture to concurrently calculate dozens of pixels simultaneously.
The first step for any pixel is calculating its corresponding ray using the equations detailed in the background section.
From there, the pixel and ray components are passed into the first ray-marching core.
Each ray marching core is responsible for one iteration of the ray marching algorithm.
Because the algorithm itself takes up multiple clock cycles and the SDF evaluation can take up many more, the entire core operates as a long pipeline.
One pixel/ray combo is passed in per clock cycle, and, once per clock cycle, the core spits out the same pixel and ray information coupled with a new value of <eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span></span></span></span></eq>.
The cores can be chained together to achieve multiple stages of iteration per clock cycle.</p>
<p><img src="./assets/pipeline_architecture.svg" alt="Simplified pipeline architecture"></p>
<p><em>Figure X: Simplified pipeline architecture</em></p>
<p>The number of cores that can be chained together is dependent on the available hardware on the FPGA.
Because calculating more complicated SDFs takes additional hardware, the number of stages is also inversely proportional to the complexity of the scene.
For simple scenes (e.g. a single cube), up to six cores can be chained together.
This drops to three or even two for more interesting scenes and quickly becomes detrimental to the quality of the rendering.
Truthfully, even a six-iteration ray marcher offers subpar rendering results, as shown in figure X.</p>
<p><img src="./assets/maybe_a_cube.webp" alt=""></p>
<p><em>Figure X: A &quot;cube&quot; rendered with six iterations</em></p>
<p>To do better the architecture must be revised to allow pixels to reenter the pipeline if they need further refinement.
With this strategy, each pixel/ray is evaluated in the last core to see if it has intersected the scene.
If it has, a new pixel is pushed into the pipeline and a global pixel index is incremented to represent the next pixel in line.
If it has not intersected with the scene yet, it is fed back into the first core, and the pixel index is not incremented.
This way pixels can take multiple rides through the pipeline according to their needs.</p>
<p><img src="./assets/updated_pipeline_architecture.svg" alt="Updated pipeline architecture"></p>
<p><em>Figure X: Updated pipeline architecture</em></p>
<p>This architecture provides an unbounded number of iterations to the pixels that need them, allowing for much crisper and more detailed renders.
It also provides a variable refresh rate, where low-effort pixels can continue getting rerendered while high-effort pixels render more slowly in the background.
This gives the illusion of snappy responsiveness during camera movement while allowing for high detail on close inspection.
As a final benefit, it allows for rendering high-complexity scenes that would otherwise require too many resources to render in a single pipeline.</p>
<h3>Handling Pipelines Without Going Insane</h3>
<p>The floating point library I used requires two cycles for a floating point add and five cycles for an inverse square root.
This introduced pipelining requirements all over the project in order to keep relevant data available at the correct cycle.
Hand writing each pipeline stage would be infurating and error prone, so we adopted a system that to handle pipeline registers for us using generate statements. As an example:</p>
<pre><code>module FP_sqrt #(
    parameter PIPELINE_STAGES = 4
) (
    input i_clk,
    input [26:0] i_a,
    output [26:0] o_sqrt
);
    wire [26:0] inv_sqrt;
    reg  [26:0] i_a_pipe [PIPELINE_STAGES:0];
    FpInvSqrt inv_sq (
        .iCLK(i_clk),
        .iA(i_a),
        .oInvSqrt(inv_sqrt)
    );
    FpMul recip (
        .iA(inv_sqrt),
        .iB(i_a_pipe[PIPELINE_STAGES]),
        .oProd(o_sqrt)
    );
    genvar i;
    generate
        for (i = 0; i &lt; PIPELINE_STAGES; i = i + 1) begin : g_ray_pipeline
            always @(posedge i_clk) begin
                i_a_pipe[0] = i_a;
                i_a_pipe[i+1] &lt;= i_a_pipe[i];
            end
        end
    endgenerate
endmodule
</code></pre>
<p>This snippet creates a series of pipeline registers of length <code>PIPELINE_STAGES</code>, and pushes data through the pipeline on each clock cycle.
This is required because input <code>i_a</code> will be updated every cycle, and will have a new value by the time <code>FpInvSqrt</code>'s output becomes valid for use in the multiply.
This snippet is a trivial example, but the system was crucial for other parts of the code that must pipeline up to ten diffent values.</p>
<h3>Pixel to Ray</h3>
<p>As mentioned in the background section, the ray corresponding to a pixel can be calculated in the following way:</p>
<pre><code>vec2 xy = coordinate.xy - resolution.xy / 2.0;
float z = resolution.y / tan(radians(field_of_view) / 2.0);
vec3 ray = normalize(vec3(xy, -z));
</code></pre>
<p>This assumes a camera that always looks straight forward, however, which is a bit boring. To spice things up, we added a <a href="https://medium.com/@carmencincotti/lets-look-at-magic-lookat-matrices-c77e53ebdf78">look at matrix</a>. Because the camera movement will be computed on the HPS, we computed the matrix in C and transfered it over PIO to the FPGA.</p>
<pre><code>vec2 xy = coordinate.xy - resolution.xy / 2.0;
float z = resolution.y / tan(radians(FIELD_OF_VIEW) / 2.0);
vec3 ray = lookAt(
    -camera_pos,
    vec3(0.0, 0.0, 0.0),
    vec3(0.0, 1.0, 0.0)
) * normalize(vec3(xy, -z));
</code></pre>
<p>At first glance these operations seem intimidating the execute on an FPGA, specifically the divisions and tan operation.
However, <code>resolution</code> and <code>field_of_view</code> are both constant, so anything involving them can be precomputed. In fact, the entire <code>z</code> assignment boils down to a constant.
All other operations are linear, and can be computed with the vector math library we created. Calculating a ray takes 9 cycles, but is pipelined. Here's what the operation looks like in Verilog:</p>
<pre><code>wire signed [`CORDW:0] x_signed, y_signed, x_adj, y_adj;
assign x_signed = {1'b0, i_x};
assign y_signed = {1'b0, i_y};

assign x_adj = x_signed - (`SCREEN_WIDTH &gt;&gt; 1);
assign y_adj = y_signed - (`SCREEN_HEIGHT &gt;&gt; 1);
wire [26:0] x_fp, y_fp, z_fp, res_x_fp, res_y_fp;
Int2Fp px_fp (
    .iInteger({{5{x_adj[`CORDW]}}, x_adj[`CORDW:0]}),
    .oA(x_fp)
);
Int2Fp py_fp (
    .iInteger({{5{y_adj[`CORDW]}}, y_adj[`CORDW:0]}),
    .oA(y_fp)
);
Int2Fp calc_res_x_fp (
    .iInteger(`SCREEN_WIDTH),
    .oA(res_x_fp)
);
Int2Fp calc_res_y_fp (
    .iInteger(`SCREEN_HEIGHT),
    .oA(res_y_fp)
);
FpMul z_calc (
    .iA(res_y_fp),
    .iB(`FOV_MAGIC_NUMBER),
    .oProd(z_fp)
);
wire [26:0] x_norm_fp, y_norm_fp, z_norm_fp;
VEC_normalize hi (
    .i_clk(i_clk),
    .i_x(x_fp),
    .i_y(y_fp),
    .i_z(z_fp),
    .o_norm_x(x_norm_fp),
    .o_norm_y(y_norm_fp),
    .o_norm_z(z_norm_fp)
);
wire [26:0] z_neg_fp;
FpNegate negate_z (
    .iA(z_norm_fp),
    .oNegative(z_neg_fp)
);
VEC_3x3_mult oh_god (
    .i_clk(i_clk),
    .i_m_1_1(look_at_1_1),
    .i_m_1_2(look_at_1_2),
    .i_m_1_3(look_at_1_3),
    .i_m_2_1(look_at_2_1),
    .i_m_2_2(look_at_2_2),
    .i_m_2_3(look_at_2_3),
    .i_m_3_1(look_at_3_1),
    .i_m_3_2(look_at_3_2),
    .i_m_3_3(look_at_3_3),
    .i_x(x_norm_fp),
    .i_y(y_norm_fp),
    .i_z(z_neg_fp),
    .o_x(o_x),
    .o_y(o_y),
    .o_z(o_z)
);
</code></pre>
<p>From here on out we won't show full verilog functions, instead showing block diagrams of their functions.
If you're interested in the full code, you can find it on <a href="https://github.com/usedhondacivic/fractal_gpu">Michael’s GitHub page</a>.</p>
<h3>Ray marching core</h3>
<p>The ray marching core is build from the following GLSL model:</p>
<pre><code>rayInfo raymarch() {
    vec3 dir = getPixelRay();
    float depth = MIN_DIST;
    for (int i = 0; i &lt; MAX_MARCHING_STEPS; i++) {
        float dist = sceneSDF(u_camera + depth * dir);
        if (dist &lt; EPSILON) {
            return rayInfo(vec3(1.0, 1.0, 1.0)));
        }
        depth += dist;
        if (depth &gt;= MAX_DIST) {
            return rayInfo(vec3(0.0, 0.0, 0.0));
        }
    }
    return rayInfo(vec3(0.0, 0.0, 0.0));
}
</code></pre>
<p>The main pain point here is the sheer number of inputs that need to be pipelined and kept in synchronization.
Between ray marching cores, each of which represents one iteration of the for loop, we must keep track of:</p>
<ul>
<li>current point (x, y, z)</li>
<li>current depth</li>
<li>pixel location (x, y)</li>
<li>ray corresponding to that pixel (x, y, z)</li>
</ul>
<p>On top of that, the SDF has a variable pipeline length depending on the scene, so the whole system has to be parameterized based on that quantity.
The following block diagram ignores those details, but you can find the gory details <a href="https://github.com/usedhondacivic/fractal_gpu/blob/main/verilator/raymarcher.v">in the code base</a>.</p>
<p><img src="./assets/ray_core_diagram.svg" alt="Ray marching core diagram"></p>
<p><em>Figure X: Diagram of a raymarching core.</em></p>
<h3>SDFs</h3>
<p>When calculating an SDF is similar to the other math in this write up, and a large collection of equations <a href="https://iquilezles.org/articles/distfunctions/">can be found here</a>.
To show how different the hardware requirements for two primatives can be, here's the block diagrams for a sphere vs a cube:</p>
<p><img src="./assets/sdf_sphere_diagram.svg" alt="Ray marching core diagram"></p>
<p><em>Figure X: Diagram of a sphere's SDF.</em></p>
<p><img src="./assets/sdf_cube_diagram.svg" alt="Cube SDF diagram"></p>
<p><em>Figure X: Diagram of a cube's SDF.</em></p>
<p>As mentioned in the background section, SDF's can also be combined. This, once again, becomes tricky with pipelining.
It is crutial that the faster SDF in the operation (in terms of clock cycles till a valid output) is pipelined to match the latency of the slower SDF.
We used module parameters to make our operations robust to this:</p>
<pre><code class="language-verilog">box BOX (
    .clk(clk),
    .point_x(a_x),
    .point_y(a_y),
    .point_z(a_z),
    .dim_x(`ONE),
    .dim_y(`ONE),
    .dim_z(`ONE),
    .distance(cube_dist)
);
sphere BALL (
    .clk(clk),
    .point_x(a_x),
    .point_y(a_y),
    .point_z(a_z),
    .radius(`ONE_POINT_THREE),
    .distance(sphere_dist)
);
sdf_union #(
    .SDF_A_PIPELINE_CYCLES(9),
    .SDF_B_PIPELINE_CYCLES(11),
) UNION (
    .clk(clk),
    .i_dist_a(sphere_dist),
    .i_dist_b(cube_dist),
    .o_dist(distance)
);
</code></pre>
<p>Verilog code for all of the SDFs I used and some basic operations (union, difference) <a href="https://github.com/usedhondacivic/fractal_gpu/tree/main/verilator/sdf">can be found here</a>.</p>
<h3>Color calculation and the VGA driver</h3>
<p>The ray marching pipeline outputs general information about the scene, but another step is required to translate that information into a pixel color.
We abstracted this mapping into a module that takes in a distance from the ray marcher, and outputs a color.
This is a rather simplistic system, and most ray marching renderers utilize other information like iterations before converence, surface normals, and material ID's to decide on a final color.
For our application however, we decided it was best to keep it simple.</p>
<p>To choose the color of a pixel, the floating point distance is shifted left by a certain amount for each color channel.
The float is then converted into an int, and the lower bits are taken as value for that color.
The shift amount is a parameter read over PIO, allowing the HPS to change the mood of the render through adjustments to the periodicity of each color.
For additional control, we also have flags to selectively turn off each channel.</p>
<h2>Bugs, issues, and future work</h2>
<p>This project was really pushed the FPGA to it's limits, and it unsuprisingly began to come appart at the seams.
This is evident when you watch the demos of some more complicated scenes, and notice the intense artifacting.
Because simulation shows much better results, we believe this has something to do with violating the timing constraints of the FGPA fabric, although there are likely several bugs in our code.</p>
<p>Nonetheless, the project was ideated as a way of conveying emotion through mixed media, and glitches or not, we think it was very effective at achieving that goal.</p>
<h2>Appendix</h2>
<h3>Permissions</h3>
<p>The group approves this report for inclusion on the course website.</p>
<p>The group approves the video for inclusion on the course youtube channel.</p>
<h3>Code listing</h3>
<p><a href="https://github.com/usedhondacivic/fractal_gpu/tree/main/verilator/sdf">Complete source code</a></p>
<h3>Specific Tasks</h3>
<p>Michael:</p>
<ul>
<li>Prototyping and development of the GPU's Verilog</li>
<li>Integration testing with the FPGA</li>
<li>Camera controls and movement sequences</li>
</ul>
<p>Antti:</p>
<ul>
<li>Train and deploy machine learning model to perform sentiment anaylsis on songs</li>
<li>Apply the model to affect models and movement within the graphics module to evoke the appropriate emotion for the song</li>
</ul>
<h3>References</h3>

	</article>
	<div id="right_pad"></div>
</body>

</html>
