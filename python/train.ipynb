{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (fc1): Linear(in_features=2, out_features=10, bias=True)\n",
      "  (fc2): Linear(in_features=10, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import extract\n",
    "import random\n",
    "\n",
    "\n",
    "# Define the logistic regression model\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(2, 1) \n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 10)\n",
    "        self.fc2 = nn.Linear(10, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))    # Pass input through first hidden layer and activation function\n",
    "        x = self.fc2(x)\n",
    "        #return torch.sigmoid(x)\n",
    "        return F.softmax(x)\n",
    "\n",
    "class NeuralNetwork2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork2, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 10)\n",
    "        self.fc2 = nn.Linear(10, 5)\n",
    "        self.fc3 = nn.Linear(5, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))    # Pass input through first hidden layer and activation function# Output layer\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        #return torch.sigmoid(x)\n",
    "        return F.softmax(x)\n",
    "\n",
    "# Example usage:\n",
    "# Create an instance of the NeuralNetwork class\n",
    "#model = LogisticRegression()\n",
    "model = NeuralNetwork()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(extract.get_tensor_from_mood_label(\"Angry\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(extract.extract_features_from_uri(\"6mVD1SfTvlFAPVi7txFL5H\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor([0.6490, 0.6450]), tensor([1., 0., 0.])), (tensor([0.2970, 0.1850]), tensor([0., 0., 1.])), (tensor([0.9630, 0.2110]), tensor([0., 0., 1.])), (tensor([0.7240, 0.1500]), tensor([1., 0., 0.])), (tensor([0.5010, 0.3330]), tensor([0., 0., 1.])), (tensor([0.7150, 0.4970]), tensor([0., 1., 0.])), (tensor([0.9020, 0.3060]), tensor([0., 1., 0.])), (tensor([0.7100, 0.8450]), tensor([1., 0., 0.])), (tensor([0.8960, 0.5670]), tensor([1., 0., 0.])), (tensor([0.9440, 0.7350]), tensor([0., 1., 0.])), (tensor([0.4300, 0.1040]), tensor([0., 0., 1.])), (tensor([0.3760, 0.0723]), tensor([0., 0., 1.])), (tensor([0.4810, 0.0773]), tensor([0., 0., 1.])), (tensor([0.5210, 0.6790]), tensor([1., 0., 0.])), (tensor([0.7210, 0.5570]), tensor([1., 0., 0.])), (tensor([0.5750, 0.9640]), tensor([1., 0., 0.])), (tensor([0.7490, 0.6490]), tensor([1., 0., 0.])), (tensor([0.5320, 0.3670]), tensor([0., 0., 1.])), (tensor([0.6170, 0.5430]), tensor([1., 0., 0.])), (tensor([0.8530, 0.1190]), tensor([0., 0., 1.])), (tensor([0.9000, 0.5610]), tensor([1., 0., 0.])), (tensor([0.3650, 0.2190]), tensor([0., 0., 1.])), (tensor([0.5570, 0.5060]), tensor([0., 0., 1.])), (tensor([0.6960, 0.5130]), tensor([1., 0., 0.])), (tensor([0.6150, 0.1550]), tensor([0., 0., 1.])), (tensor([0.7780, 0.3040]), tensor([0., 1., 0.])), (tensor([0.9030, 0.1450]), tensor([0., 1., 0.])), (tensor([0.4520, 0.7400]), tensor([0., 0., 1.])), (tensor([0.3560, 0.3380]), tensor([0., 0., 1.])), (tensor([0.2290, 0.1350]), tensor([0., 0., 1.])), (tensor([0.8410, 0.7060]), tensor([1., 0., 0.])), (tensor([0.8490, 0.2560]), tensor([0., 1., 0.])), (tensor([0.4590, 0.3830]), tensor([0., 0., 1.])), (tensor([0.6800, 0.7370]), tensor([1., 0., 0.])), (tensor([0.7940, 0.9310]), tensor([1., 0., 0.])), (tensor([0.5780, 0.6540]), tensor([1., 0., 0.])), (tensor([0.3420, 0.1670]), tensor([0., 0., 1.])), (tensor([0.6470, 0.5520]), tensor([1., 0., 0.])), (tensor([0.1300, 0.0427]), tensor([0., 0., 1.])), (tensor([0.7940, 0.6250]), tensor([1., 0., 0.])), (tensor([0.6170, 0.2010]), tensor([0., 0., 1.])), (tensor([0.8880, 0.3290]), tensor([0., 1., 0.])), (tensor([0.6920, 0.8460]), tensor([1., 0., 0.])), (tensor([0.9000, 0.4910]), tensor([0., 1., 0.])), (tensor([0.6660, 0.6460]), tensor([1., 0., 0.])), (tensor([0.4490, 0.5720]), tensor([0., 0., 0.])), (tensor([0.9010, 0.6660]), tensor([0., 1., 0.])), (tensor([0.6000, 0.6530]), tensor([1., 0., 0.])), (tensor([0.3730, 0.2530]), tensor([0., 0., 1.])), (tensor([0.3290, 0.0808]), tensor([0., 0., 1.])), (tensor([0.8100, 0.6610]), tensor([1., 0., 0.])), (tensor([0.7520, 0.1150]), tensor([0., 0., 1.])), (tensor([0.7720, 0.2940]), tensor([0., 1., 0.])), (tensor([0.3880, 0.6390]), tensor([1., 0., 0.])), (tensor([0.6920, 0.3640]), tensor([0., 0., 1.])), (tensor([0.6770, 0.3710]), tensor([0., 1., 0.])), (tensor([0.3410, 0.0380]), tensor([0., 0., 1.])), (tensor([0.9290, 0.5560]), tensor([1., 0., 0.])), (tensor([0.8470, 0.2900]), tensor([0., 1., 0.])), (tensor([0.8480, 0.2070]), tensor([0., 1., 0.])), (tensor([0.2940, 0.0402]), tensor([0., 0., 1.])), (tensor([0.8490, 0.9170]), tensor([1., 0., 0.])), (tensor([0.9200, 0.2250]), tensor([0., 0., 1.])), (tensor([0.3930, 0.1180]), tensor([0., 0., 1.])), (tensor([0.9780, 0.5030]), tensor([0., 1., 0.])), (tensor([0.7270, 0.7480]), tensor([1., 0., 0.])), (tensor([0.5830, 0.9080]), tensor([1., 0., 0.])), (tensor([0.5300, 0.0586]), tensor([0., 0., 1.])), (tensor([0.3940, 0.7210]), tensor([0., 1., 0.])), (tensor([0.7390, 0.5310]), tensor([1., 0., 0.])), (tensor([0.9310, 0.1720]), tensor([0., 1., 0.])), (tensor([0.9390, 0.6290]), tensor([1., 0., 0.])), (tensor([0.3080, 0.4140]), tensor([0., 0., 1.])), (tensor([0.5360, 0.7850]), tensor([1., 0., 0.])), (tensor([0.3410, 0.1780]), tensor([0., 0., 1.])), (tensor([0.7320, 0.7340]), tensor([1., 0., 0.])), (tensor([0.6310, 0.4600]), tensor([0., 1., 0.])), (tensor([0.6760, 0.1430]), tensor([0., 0., 1.])), (tensor([0.6720, 0.5050]), tensor([1., 0., 0.])), (tensor([0.8030, 0.9690]), tensor([1., 0., 0.])), (tensor([0.6050, 0.7560]), tensor([1., 0., 0.])), (tensor([0.6910, 0.2060]), tensor([0., 1., 0.])), (tensor([0.6270, 0.8140]), tensor([1., 0., 0.])), (tensor([0.7620, 0.4610]), tensor([1., 0., 0.])), (tensor([0.5940, 0.2660]), tensor([0., 0., 1.])), (tensor([0.5880, 0.9620]), tensor([1., 0., 0.])), (tensor([0.8250, 0.4790]), tensor([0., 1., 0.])), (tensor([0.6820, 0.3960]), tensor([0., 1., 0.])), (tensor([0.7890, 0.9180]), tensor([1., 0., 0.])), (tensor([0.8260, 0.4150]), tensor([0., 1., 0.])), (tensor([0.7750, 0.5880]), tensor([1., 0., 0.])), (tensor([0.7770, 0.2350]), tensor([0., 0., 1.])), (tensor([0.9620, 0.8140]), tensor([1., 0., 0.])), (tensor([0.6430, 0.5140]), tensor([0., 0., 0.])), (tensor([0.9010, 0.8390]), tensor([1., 0., 0.])), (tensor([0.9190, 0.5370]), tensor([1., 0., 0.])), (tensor([0.7620, 0.5050]), tensor([1., 0., 0.])), (tensor([0.6550, 0.5310]), tensor([1., 0., 0.])), (tensor([0.7200, 0.4750]), tensor([0., 1., 0.])), (tensor([0.9250, 0.3690]), tensor([1., 0., 0.])), (tensor([0.7220, 0.7160]), tensor([1., 0., 0.])), (tensor([0.0301, 0.0256]), tensor([0., 0., 1.])), (tensor([0.5320, 0.6800]), tensor([1., 0., 0.])), (tensor([0.5590, 0.9180]), tensor([1., 0., 0.])), (tensor([0.7140, 0.2390]), tensor([0., 1., 0.])), (tensor([0.3830, 0.0506]), tensor([0., 0., 1.])), (tensor([0.8870, 0.8710]), tensor([1., 0., 0.])), (tensor([0.7140, 0.2990]), tensor([0., 1., 0.])), (tensor([0.8730, 0.5620]), tensor([1., 0., 0.])), (tensor([0.8670, 0.9180]), tensor([1., 0., 0.])), (tensor([0.8220, 0.1900]), tensor([0., 1., 0.])), (tensor([0.8740, 0.3180]), tensor([0., 1., 0.])), (tensor([0.9000, 0.5330]), tensor([1., 0., 0.])), (tensor([0.9300, 0.6230]), tensor([1., 0., 0.])), (tensor([0.2620, 0.3750]), tensor([0., 0., 1.])), (tensor([0.8090, 0.7530]), tensor([1., 0., 0.])), (tensor([0.7880, 0.7080]), tensor([1., 0., 0.])), (tensor([0.7210, 0.7630]), tensor([1., 0., 0.])), (tensor([0.6130, 0.2820]), tensor([0., 0., 1.])), (tensor([0.9270, 0.3850]), tensor([0., 1., 0.])), (tensor([0.8080, 0.8960]), tensor([1., 0., 0.])), (tensor([0.6680, 0.3380]), tensor([0., 1., 0.])), (tensor([0.9560, 0.7150]), tensor([1., 0., 0.])), (tensor([0.7650, 0.5670]), tensor([1., 0., 0.])), (tensor([0.7400, 0.7850]), tensor([1., 0., 0.])), (tensor([0.1270, 0.3720]), tensor([0., 0., 1.])), (tensor([0.6700, 0.7400]), tensor([1., 0., 0.])), (tensor([0.5240, 0.3480]), tensor([0., 0., 1.])), (tensor([0.4810, 0.1760]), tensor([0., 0., 1.])), (tensor([0.4800, 0.3550]), tensor([0., 0., 1.])), (tensor([0.9080, 0.9680]), tensor([1., 0., 0.])), (tensor([0.4720, 0.2190]), tensor([0., 0., 1.])), (tensor([0.7800, 0.5860]), tensor([1., 0., 0.])), (tensor([0.5700, 0.8770]), tensor([1., 0., 0.])), (tensor([0.9550, 0.8690]), tensor([1., 0., 0.])), (tensor([0.9510, 0.5510]), tensor([1., 0., 0.])), (tensor([0.5600, 0.5770]), tensor([1., 0., 0.])), (tensor([0.6030, 0.5390]), tensor([1., 0., 0.])), (tensor([0.6300, 0.4890]), tensor([0., 1., 0.])), (tensor([0.8770, 0.3020]), tensor([0., 1., 0.])), (tensor([0.2650, 0.2470]), tensor([0., 0., 1.])), (tensor([0.8590, 0.5110]), tensor([1., 0., 0.])), (tensor([0.7280, 0.9660]), tensor([1., 0., 0.])), (tensor([0.7390, 0.7750]), tensor([1., 0., 0.])), (tensor([0.3490, 0.0644]), tensor([0., 0., 1.])), (tensor([0.9130, 0.9330]), tensor([1., 0., 0.])), (tensor([0.7720, 0.7170]), tensor([1., 0., 0.])), (tensor([0.9120, 0.7200]), tensor([1., 0., 0.])), (tensor([0.7660, 0.8030]), tensor([1., 0., 0.])), (tensor([0.9440, 0.2650]), tensor([0., 0., 1.])), (tensor([0.7600, 0.4910]), tensor([0., 1., 0.])), (tensor([0.7420, 0.1970]), tensor([0., 1., 0.])), (tensor([0.6480, 0.4900]), tensor([0., 1., 0.])), (tensor([0.7760, 0.4080]), tensor([1., 0., 0.])), (tensor([0.5770, 0.1410]), tensor([0., 0., 1.])), (tensor([0.3380, 0.3770]), tensor([0., 0., 1.])), (tensor([0.9380, 0.9640]), tensor([1., 0., 0.])), (tensor([0.7870, 0.9640]), tensor([1., 0., 0.])), (tensor([0.6570, 0.4340]), tensor([0., 1., 0.])), (tensor([0.7120, 0.6200]), tensor([1., 0., 0.])), (tensor([0.7170, 0.5470]), tensor([1., 0., 0.])), (tensor([0.8450, 0.3910]), tensor([0., 1., 0.])), (tensor([0.7570, 0.5520]), tensor([1., 0., 0.])), (tensor([0.9410, 0.5870]), tensor([1., 0., 0.])), (tensor([0.8900, 0.8850]), tensor([1., 0., 0.])), (tensor([0.8810, 0.3640]), tensor([0., 1., 0.])), (tensor([0.4710, 0.6310]), tensor([0., 0., 1.])), (tensor([0.6090, 0.7340]), tensor([1., 0., 0.])), (tensor([0.9340, 0.2870]), tensor([0., 1., 0.])), (tensor([0.9710, 0.7640]), tensor([1., 0., 0.])), (tensor([0.9880, 0.3210]), tensor([0., 1., 0.])), (tensor([0.9050, 0.3740]), tensor([0., 1., 0.])), (tensor([0.3020, 0.1900]), tensor([0., 0., 1.])), (tensor([0.4730, 0.3850]), tensor([0., 0., 1.])), (tensor([0.8100, 0.2450]), tensor([0., 1., 0.])), (tensor([0.2630, 0.1130]), tensor([0., 0., 1.])), (tensor([0.7480, 0.9720]), tensor([1., 0., 0.])), (tensor([0.7250, 0.8080]), tensor([1., 0., 0.])), (tensor([0.3020, 0.0332]), tensor([0., 0., 1.])), (tensor([0.6810, 0.2850]), tensor([1., 0., 0.])), (tensor([0.4730, 0.1410]), tensor([0., 0., 1.])), (tensor([0.3360, 0.0679]), tensor([0., 0., 1.])), (tensor([0.9130, 0.5890]), tensor([1., 0., 0.])), (tensor([0.2410, 0.3590]), tensor([0., 0., 1.])), (tensor([0.8270, 0.6490]), tensor([0., 1., 0.])), (tensor([0.9310, 0.5250]), tensor([0., 1., 0.])), (tensor([0.5630, 0.1340]), tensor([0., 0., 1.])), (tensor([0.8130, 0.8210]), tensor([1., 0., 0.])), (tensor([0.7210, 0.2250]), tensor([0., 1., 0.])), (tensor([0.7010, 0.6900]), tensor([1., 0., 0.])), (tensor([0.8240, 0.7770]), tensor([1., 0., 0.])), (tensor([0.7250, 0.9040]), tensor([1., 0., 0.])), (tensor([0.8170, 0.8740]), tensor([1., 0., 0.])), (tensor([0.6630, 0.6630]), tensor([0., 1., 0.])), (tensor([0.6320, 0.1890]), tensor([0., 0., 1.])), (tensor([0.8680, 0.7360]), tensor([1., 0., 0.])), (tensor([0.5760, 0.4570]), tensor([0., 1., 0.])), (tensor([0.5940, 0.7340]), tensor([1., 0., 0.])), (tensor([0.0968, 0.0395]), tensor([0., 0., 1.])), (tensor([0.5060, 0.8310]), tensor([1., 0., 0.])), (tensor([0.2240, 0.1390]), tensor([0., 0., 1.])), (tensor([0.6350, 0.0767]), tensor([0., 1., 0.])), (tensor([0.5960, 0.5840]), tensor([1., 0., 0.])), (tensor([0.6490, 0.3120]), tensor([0., 1., 0.])), (tensor([0.4630, 0.0629]), tensor([0., 0., 1.])), (tensor([0.7060, 0.4830]), tensor([0., 1., 0.])), (tensor([0.7250, 0.4400]), tensor([0., 1., 0.])), (tensor([0.1320, 0.0381]), tensor([0., 0., 1.])), (tensor([0.4240, 0.4040]), tensor([0., 0., 1.])), (tensor([0.8140, 0.4840]), tensor([0., 1., 0.])), (tensor([0.5750, 0.8500]), tensor([1., 0., 0.])), (tensor([0.7880, 0.3570]), tensor([0., 1., 0.])), (tensor([0.6200, 0.4720]), tensor([0., 0., 1.])), (tensor([0.8680, 0.9230]), tensor([1., 0., 0.])), (tensor([0.6970, 0.7740]), tensor([1., 0., 0.])), (tensor([0.7880, 0.4720]), tensor([1., 0., 0.])), (tensor([0.6250, 0.8260]), tensor([1., 0., 0.])), (tensor([0.7160, 0.0618]), tensor([0., 1., 0.])), (tensor([0.5080, 0.4030]), tensor([0., 0., 1.])), (tensor([0.3860, 0.1940]), tensor([0., 0., 1.])), (tensor([0.8750, 0.4070]), tensor([0., 1., 0.])), (tensor([0.6490, 0.7700]), tensor([1., 0., 0.])), (tensor([0.5840, 0.7530]), tensor([1., 0., 0.])), (tensor([0.5210, 0.3370]), tensor([0., 0., 1.])), (tensor([0.8770, 0.8020]), tensor([1., 0., 0.])), (tensor([0.8890, 0.5920]), tensor([1., 0., 0.])), (tensor([0.7370, 0.5270]), tensor([1., 0., 0.])), (tensor([0.9800, 0.4350]), tensor([0., 1., 0.])), (tensor([0.5710, 0.4670]), tensor([1., 0., 0.])), (tensor([0.8870, 0.3560]), tensor([1., 0., 0.])), (tensor([0.5030, 0.3800]), tensor([0., 0., 1.])), (tensor([0.7450, 0.6290]), tensor([1., 0., 0.])), (tensor([0.8350, 0.4570]), tensor([1., 0., 0.])), (tensor([0.9390, 0.7410]), tensor([0., 1., 0.])), (tensor([0.8050, 0.7080]), tensor([1., 0., 0.])), (tensor([0.8330, 0.6610]), tensor([0., 1., 0.])), (tensor([0.8580, 0.5470]), tensor([1., 0., 0.])), (tensor([0.6630, 0.5270]), tensor([1., 0., 0.])), (tensor([0.9650, 0.3490]), tensor([0., 1., 0.])), (tensor([0.5560, 0.7460]), tensor([0., 1., 0.])), (tensor([0.8900, 0.4120]), tensor([1., 0., 0.])), (tensor([0.8730, 0.4800]), tensor([0., 1., 0.])), (tensor([0.8490, 0.9030]), tensor([1., 0., 0.])), (tensor([0.8850, 0.8430]), tensor([1., 0., 0.])), (tensor([0.9130, 0.7450]), tensor([1., 0., 0.])), (tensor([0.7310, 0.6970]), tensor([1., 0., 0.])), (tensor([0.7460, 0.4030]), tensor([0., 1., 0.])), (tensor([0.7280, 0.9750]), tensor([1., 0., 0.])), (tensor([0.9060, 0.3800]), tensor([0., 1., 0.])), (tensor([0.8810, 0.3940]), tensor([0., 1., 0.])), (tensor([0.4350, 0.1780]), tensor([0., 0., 1.])), (tensor([0.9110, 0.7840]), tensor([1., 0., 0.])), (tensor([0.8520, 0.2340]), tensor([0., 0., 1.])), (tensor([0.7530, 0.4200]), tensor([0., 1., 0.])), (tensor([0.7010, 0.5830]), tensor([1., 0., 0.])), (tensor([0.8210, 0.2610]), tensor([1., 0., 0.])), (tensor([0.7240, 0.2500]), tensor([1., 0., 0.])), (tensor([0.5680, 0.9520]), tensor([1., 0., 0.])), (tensor([0.2930, 0.3220]), tensor([0., 0., 1.])), (tensor([0.3220, 0.1380]), tensor([0., 0., 1.])), (tensor([0.4670, 0.7640]), tensor([1., 0., 0.])), (tensor([0.7580, 0.4350]), tensor([0., 1., 0.])), (tensor([0.8050, 0.5640]), tensor([0., 1., 0.])), (tensor([0.8930, 0.4060]), tensor([0., 1., 0.])), (tensor([0.4320, 0.6480]), tensor([1., 0., 0.])), (tensor([0.8350, 0.2700]), tensor([0., 1., 0.])), (tensor([0.6710, 0.0681]), tensor([0., 0., 1.])), (tensor([0.7510, 0.7890]), tensor([1., 0., 0.])), (tensor([0.6100, 0.1990]), tensor([0., 0., 1.])), (tensor([0.5010, 0.3240]), tensor([0., 0., 1.])), (tensor([0.7070, 0.7800]), tensor([1., 0., 0.])), (tensor([0.4920, 0.7890]), tensor([1., 0., 0.])), (tensor([0.6980, 0.3080]), tensor([0., 1., 0.])), (tensor([0.9530, 0.5370]), tensor([1., 0., 0.])), (tensor([0.6330, 0.0805]), tensor([0., 0., 1.])), (tensor([0.9600, 0.9780]), tensor([1., 0., 0.])), (tensor([0.5850, 0.7840]), tensor([1., 0., 0.])), (tensor([0.7670, 0.3590]), tensor([0., 1., 0.])), (tensor([0.6230, 0.2980]), tensor([0., 1., 0.])), (tensor([0.6540, 0.2350]), tensor([0., 0., 1.])), (tensor([0.0195, 0.0313]), tensor([0., 0., 1.])), (tensor([0.3020, 0.1900]), tensor([0., 0., 1.])), (tensor([0.9460, 0.5470]), tensor([0., 1., 0.])), (tensor([0.6320, 0.3590]), tensor([0., 0., 1.])), (tensor([0.3260, 0.0497]), tensor([0., 0., 1.])), (tensor([0.9020, 0.4970]), tensor([0., 1., 0.])), (tensor([0.5470, 0.8000]), tensor([1., 0., 0.])), (tensor([0.9220, 0.3930]), tensor([0., 1., 0.])), (tensor([0.4660, 0.1580]), tensor([0., 0., 1.])), (tensor([0.4780, 0.0997]), tensor([0., 0., 1.])), (tensor([0.5690, 0.5300]), tensor([0., 0., 1.])), (tensor([0.4890, 0.3730]), tensor([1., 0., 0.])), (tensor([0.9420, 0.9640]), tensor([1., 0., 0.])), (tensor([0.3660, 0.1710]), tensor([0., 0., 1.])), (tensor([0.9320, 0.9710]), tensor([1., 0., 0.])), (tensor([0.9440, 0.6060]), tensor([1., 0., 0.])), (tensor([0.7700, 0.8490]), tensor([1., 0., 0.])), (tensor([0.4300, 0.5470]), tensor([0., 0., 1.])), (tensor([0.8690, 0.6960]), tensor([1., 0., 0.])), (tensor([0.4430, 0.1490]), tensor([0., 0., 1.])), (tensor([0.9530, 0.4460]), tensor([0., 1., 0.])), (tensor([0.6730, 0.1870]), tensor([0., 0., 1.])), (tensor([0.2240, 0.0388]), tensor([0., 0., 1.])), (tensor([0.6080, 0.6370]), tensor([1., 0., 0.])), (tensor([0.6290, 0.7200]), tensor([1., 0., 0.])), (tensor([0.7890, 0.7150]), tensor([1., 0., 0.])), (tensor([0.9050, 0.4860]), tensor([1., 0., 0.])), (tensor([0.7060, 0.7680]), tensor([1., 0., 0.])), (tensor([0.5850, 0.5180]), tensor([0., 0., 1.])), (tensor([0.3830, 0.3500]), tensor([0., 0., 1.])), (tensor([0.6580, 0.8500]), tensor([1., 0., 0.])), (tensor([0.8440, 0.5590]), tensor([1., 0., 0.])), (tensor([0.6000, 0.8000]), tensor([1., 0., 0.])), (tensor([0.3180, 0.3850]), tensor([0., 0., 1.])), (tensor([0.3350, 0.1410]), tensor([0., 0., 1.])), (tensor([0.4300, 0.0403]), tensor([0., 0., 1.])), (tensor([0.9320, 0.6190]), tensor([1., 0., 0.])), (tensor([0.5460, 0.4290]), tensor([0., 0., 1.])), (tensor([0.6550, 0.3680]), tensor([0., 0., 1.])), (tensor([0.4110, 0.6750]), tensor([1., 0., 0.])), (tensor([0.7140, 0.4480]), tensor([0., 1., 0.])), (tensor([0.9050, 0.4860]), tensor([0., 1., 0.])), (tensor([0.2650, 0.5220]), tensor([0., 0., 1.])), (tensor([0.3840, 0.2160]), tensor([0., 0., 1.])), (tensor([0.8980, 0.5970]), tensor([1., 0., 0.])), (tensor([0.7260, 0.5070]), tensor([1., 0., 0.])), (tensor([0.8460, 0.2870]), tensor([0., 0., 1.])), (tensor([0.5900, 0.8180]), tensor([1., 0., 0.])), (tensor([0.7870, 0.5070]), tensor([1., 0., 0.])), (tensor([0.4790, 0.1400]), tensor([0., 0., 1.])), (tensor([0.6790, 0.0888]), tensor([0., 0., 1.])), (tensor([0.6680, 0.5610]), tensor([1., 0., 0.])), (tensor([0.9170, 0.2270]), tensor([0., 1., 0.])), (tensor([0.2390, 0.1470]), tensor([0., 0., 1.])), (tensor([0.6960, 0.8490]), tensor([1., 0., 0.])), (tensor([0.7140, 0.1910]), tensor([0., 1., 0.])), (tensor([0.7900, 0.4660]), tensor([0., 1., 0.])), (tensor([0.2260, 0.0480]), tensor([0., 0., 1.])), (tensor([0.3230, 0.3630]), tensor([0., 0., 1.])), (tensor([0.4540, 0.1310]), tensor([0., 0., 1.])), (tensor([0.7750, 0.8130]), tensor([1., 0., 0.])), (tensor([0.0476, 0.0222]), tensor([0., 0., 1.])), (tensor([0.8630, 0.9310]), tensor([1., 0., 0.])), (tensor([0.0585, 0.0371]), tensor([0., 0., 1.])), (tensor([0.7450, 0.6290]), tensor([1., 0., 0.])), (tensor([0.6560, 0.2140]), tensor([0., 0., 1.])), (tensor([0.4050, 0.8280]), tensor([1., 0., 0.])), (tensor([0.8940, 0.2770]), tensor([0., 1., 0.])), (tensor([0.5960, 0.1560]), tensor([0., 0., 1.])), (tensor([0.8750, 0.5950]), tensor([0., 1., 0.])), (tensor([0.8470, 0.4810]), tensor([1., 0., 0.])), (tensor([0.8320, 0.8070]), tensor([1., 0., 0.])), (tensor([0.8750, 0.5750]), tensor([0., 1., 0.])), (tensor([0.4490, 0.5490]), tensor([1., 0., 0.])), (tensor([0.6280, 0.8930]), tensor([1., 0., 0.])), (tensor([0.7110, 0.4120]), tensor([1., 0., 0.])), (tensor([0.9630, 0.8560]), tensor([1., 0., 0.])), (tensor([0.4000, 0.1950]), tensor([0., 0., 1.])), (tensor([0.5710, 0.8200]), tensor([1., 0., 0.])), (tensor([0.5510, 0.4530]), tensor([0., 0., 1.])), (tensor([0.9760, 0.7330]), tensor([1., 0., 0.])), (tensor([0.5160, 0.6710]), tensor([1., 0., 0.])), (tensor([0.3020, 0.1350]), tensor([0., 0., 1.])), (tensor([0.5100, 0.7350]), tensor([1., 0., 0.])), (tensor([0.6250, 0.5140]), tensor([1., 0., 0.])), (tensor([0.5210, 0.4940]), tensor([0., 1., 0.])), (tensor([0.5990, 0.3120]), tensor([0., 0., 1.])), (tensor([0.6960, 0.6570]), tensor([1., 0., 0.])), (tensor([0.5540, 0.9630]), tensor([1., 0., 0.])), (tensor([0.9430, 0.7430]), tensor([1., 0., 0.])), (tensor([0.5020, 0.2890]), tensor([0., 0., 1.])), (tensor([0.9290, 0.5900]), tensor([1., 0., 0.])), (tensor([0.3820, 0.7140]), tensor([1., 0., 0.])), (tensor([0.6920, 0.9520]), tensor([1., 0., 0.])), (tensor([0.4810, 0.4910]), tensor([0., 0., 1.])), (tensor([0.7660, 0.5580]), tensor([1., 0., 0.])), (tensor([0.8470, 0.6400]), tensor([1., 0., 0.])), (tensor([0.6770, 0.6840]), tensor([1., 0., 0.])), (tensor([0.3420, 0.0795]), tensor([0., 0., 1.])), (tensor([0.9680, 0.7510]), tensor([1., 0., 0.])), (tensor([0.9410, 0.4070]), tensor([0., 1., 0.])), (tensor([0.9360, 0.3100]), tensor([0., 1., 0.])), (tensor([0.8820, 0.5700]), tensor([1., 0., 0.])), (tensor([0.8180, 0.4710]), tensor([1., 0., 0.])), (tensor([0.5060, 0.3880]), tensor([0., 0., 1.])), (tensor([0.8840, 0.8520]), tensor([1., 0., 0.])), (tensor([0.7240, 0.5030]), tensor([1., 0., 0.])), (tensor([0.7460, 0.6140]), tensor([1., 0., 0.])), (tensor([0.4970, 0.5690]), tensor([1., 0., 0.])), (tensor([0.7270, 0.5780]), tensor([0., 1., 0.])), (tensor([0.8640, 0.7490]), tensor([1., 0., 0.])), (tensor([0.8330, 0.5860]), tensor([0., 1., 0.])), (tensor([0.5530, 0.5330]), tensor([1., 0., 0.])), (tensor([0.8030, 0.2210]), tensor([0., 1., 0.])), (tensor([0.7500, 0.4010]), tensor([0., 1., 0.])), (tensor([0.6210, 0.7820]), tensor([1., 0., 0.])), (tensor([0.2170, 0.1810]), tensor([0., 0., 1.])), (tensor([0.2650, 0.2080]), tensor([0., 0., 1.])), (tensor([0.9150, 0.7770]), tensor([1., 0., 0.])), (tensor([0.3250, 0.6680]), tensor([1., 0., 0.])), (tensor([0.6660, 0.6660]), tensor([1., 0., 0.])), (tensor([0.2760, 0.1950]), tensor([0., 0., 1.])), (tensor([0.6230, 0.6760]), tensor([1., 0., 0.])), (tensor([0.7450, 0.8790]), tensor([1., 0., 0.])), (tensor([0.9400, 0.4210]), tensor([1., 0., 0.])), (tensor([0.8700, 0.6710]), tensor([1., 0., 0.])), (tensor([0.5750, 0.3540]), tensor([0., 0., 1.])), (tensor([0.6540, 0.7020]), tensor([1., 0., 0.])), (tensor([0.8340, 0.3380]), tensor([0., 1., 0.])), (tensor([0.5550, 0.8340]), tensor([1., 0., 0.])), (tensor([0.9470, 0.8030]), tensor([1., 0., 0.])), (tensor([0.6560, 0.7820]), tensor([1., 0., 0.])), (tensor([0.5150, 0.4200]), tensor([0., 0., 1.])), (tensor([0.4640, 0.5720]), tensor([1., 0., 0.])), (tensor([0.5580, 0.0985]), tensor([0., 0., 1.])), (tensor([0.3400, 0.1970]), tensor([0., 0., 1.])), (tensor([0.9030, 0.7690]), tensor([1., 0., 0.])), (tensor([0.4920, 0.0366]), tensor([0., 0., 1.])), (tensor([0.4850, 0.1240]), tensor([0., 0., 1.])), (tensor([0.2010, 0.0668]), tensor([0., 0., 1.])), (tensor([0.8460, 0.2210]), tensor([0., 0., 1.]))]\n"
     ]
    }
   ],
   "source": [
    "'''dataset = extract.extract_song_data(\"training_set.xlsx\")\n",
    "\n",
    "#print(dataset)\n",
    "#print(len(dataset))\n",
    "\n",
    "\n",
    "\n",
    "input_tensors = []\n",
    "for i in range(0, len(dataset), 100):\n",
    "    length = 100 if len(dataset) - i > 100 else len(dataset) - i\n",
    "    uri_lst = [uri for _, uri, _ in dataset[i : i + length]]\n",
    "    for t in extract.extract_features_from_uris(uri_lst):\n",
    "        input_tensors.append(t)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inputs_outputs = [(input_tensors[i], extract.get_tensor_from_mood_label(dataset[i][2])) for i in range(dataset)]\n",
    "training_set =random.sample(inputs_outputs, 300)\n",
    "testing_set = [x for x in dataset if x not in training_set]\n",
    "\n",
    "\n",
    "print(training_set[0:5])'''\n",
    "\n",
    "dataset = extract.get_dataset_from_spreadsheet(\"final_tset.xlsx\")\n",
    "#print(dataset)\n",
    "\n",
    "dataset = random.sample(dataset, len(dataset))\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Antti\\AppData\\Local\\Temp\\ipykernel_52420\\2662789135.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 1.1122\n",
      "Epoch [2/200], Loss: 1.1720\n",
      "Epoch [3/200], Loss: 1.2108\n",
      "Epoch [4/200], Loss: 1.2286\n",
      "Epoch [5/200], Loss: 1.2301\n",
      "Epoch [6/200], Loss: 1.2200\n",
      "Epoch [7/200], Loss: 1.2013\n",
      "Epoch [8/200], Loss: 1.1751\n",
      "Epoch [9/200], Loss: 1.1421\n",
      "Epoch [10/200], Loss: 1.1028\n",
      "Epoch [11/200], Loss: 1.0568\n",
      "Epoch [12/200], Loss: 1.0060\n",
      "Epoch [13/200], Loss: 0.9518\n",
      "Epoch [14/200], Loss: 0.8979\n",
      "Epoch [15/200], Loss: 0.8461\n",
      "Epoch [16/200], Loss: 0.7997\n",
      "Epoch [17/200], Loss: 0.7605\n",
      "Epoch [18/200], Loss: 0.7271\n",
      "Epoch [19/200], Loss: 0.7006\n",
      "Epoch [20/200], Loss: 0.6786\n",
      "Epoch [21/200], Loss: 0.6609\n",
      "Epoch [22/200], Loss: 0.6466\n",
      "Epoch [23/200], Loss: 0.6353\n",
      "Epoch [24/200], Loss: 0.6261\n",
      "Epoch [25/200], Loss: 0.6188\n",
      "Epoch [26/200], Loss: 0.6128\n",
      "Epoch [27/200], Loss: 0.6082\n",
      "Epoch [28/200], Loss: 0.6044\n",
      "Epoch [29/200], Loss: 0.6016\n",
      "Epoch [30/200], Loss: 0.5995\n",
      "Epoch [31/200], Loss: 0.5983\n",
      "Epoch [32/200], Loss: 0.5978\n",
      "Epoch [33/200], Loss: 0.5985\n",
      "Epoch [34/200], Loss: 0.5998\n",
      "Epoch [35/200], Loss: 0.6015\n",
      "Epoch [36/200], Loss: 0.6033\n",
      "Epoch [37/200], Loss: 0.6050\n",
      "Epoch [38/200], Loss: 0.6061\n",
      "Epoch [39/200], Loss: 0.6064\n",
      "Epoch [40/200], Loss: 0.6062\n",
      "Epoch [41/200], Loss: 0.6056\n",
      "Epoch [42/200], Loss: 0.6044\n",
      "Epoch [43/200], Loss: 0.6031\n",
      "Epoch [44/200], Loss: 0.6017\n",
      "Epoch [45/200], Loss: 0.6002\n",
      "Epoch [46/200], Loss: 0.5986\n",
      "Epoch [47/200], Loss: 0.5971\n",
      "Epoch [48/200], Loss: 0.5956\n",
      "Epoch [49/200], Loss: 0.5941\n",
      "Epoch [50/200], Loss: 0.5926\n",
      "Epoch [51/200], Loss: 0.5912\n",
      "Epoch [52/200], Loss: 0.5898\n",
      "Epoch [53/200], Loss: 0.5885\n",
      "Epoch [54/200], Loss: 0.5873\n",
      "Epoch [55/200], Loss: 0.5861\n",
      "Epoch [56/200], Loss: 0.5850\n",
      "Epoch [57/200], Loss: 0.5839\n",
      "Epoch [58/200], Loss: 0.5829\n",
      "Epoch [59/200], Loss: 0.5819\n",
      "Epoch [60/200], Loss: 0.5809\n",
      "Epoch [61/200], Loss: 0.5800\n",
      "Epoch [62/200], Loss: 0.5791\n",
      "Epoch [63/200], Loss: 0.5782\n",
      "Epoch [64/200], Loss: 0.5774\n",
      "Epoch [65/200], Loss: 0.5766\n",
      "Epoch [66/200], Loss: 0.5758\n",
      "Epoch [67/200], Loss: 0.5750\n",
      "Epoch [68/200], Loss: 0.5743\n",
      "Epoch [69/200], Loss: 0.5736\n",
      "Epoch [70/200], Loss: 0.5729\n",
      "Epoch [71/200], Loss: 0.5722\n",
      "Epoch [72/200], Loss: 0.5716\n",
      "Epoch [73/200], Loss: 0.5710\n",
      "Epoch [74/200], Loss: 0.5705\n",
      "Epoch [75/200], Loss: 0.5699\n",
      "Epoch [76/200], Loss: 0.5694\n",
      "Epoch [77/200], Loss: 0.5689\n",
      "Epoch [78/200], Loss: 0.5683\n",
      "Epoch [79/200], Loss: 0.5679\n",
      "Epoch [80/200], Loss: 0.5674\n",
      "Epoch [81/200], Loss: 0.5670\n",
      "Epoch [82/200], Loss: 0.5666\n",
      "Epoch [83/200], Loss: 0.5662\n",
      "Epoch [84/200], Loss: 0.5659\n",
      "Epoch [85/200], Loss: 0.5655\n",
      "Epoch [86/200], Loss: 0.5651\n",
      "Epoch [87/200], Loss: 0.5648\n",
      "Epoch [88/200], Loss: 0.5644\n",
      "Epoch [89/200], Loss: 0.5641\n",
      "Epoch [90/200], Loss: 0.5638\n",
      "Epoch [91/200], Loss: 0.5635\n",
      "Epoch [92/200], Loss: 0.5632\n",
      "Epoch [93/200], Loss: 0.5629\n",
      "Epoch [94/200], Loss: 0.5626\n",
      "Epoch [95/200], Loss: 0.5623\n",
      "Epoch [96/200], Loss: 0.5620\n",
      "Epoch [97/200], Loss: 0.5618\n",
      "Epoch [98/200], Loss: 0.5615\n",
      "Epoch [99/200], Loss: 0.5613\n",
      "Epoch [100/200], Loss: 0.5610\n",
      "Epoch [101/200], Loss: 0.5608\n",
      "Epoch [102/200], Loss: 0.5605\n",
      "Epoch [103/200], Loss: 0.5603\n",
      "Epoch [104/200], Loss: 0.5601\n",
      "Epoch [105/200], Loss: 0.5599\n",
      "Epoch [106/200], Loss: 0.5597\n",
      "Epoch [107/200], Loss: 0.5594\n",
      "Epoch [108/200], Loss: 0.5593\n",
      "Epoch [109/200], Loss: 0.5590\n",
      "Epoch [110/200], Loss: 0.5588\n",
      "Epoch [111/200], Loss: 0.5587\n",
      "Epoch [112/200], Loss: 0.5585\n",
      "Epoch [113/200], Loss: 0.5583\n",
      "Epoch [114/200], Loss: 0.5582\n",
      "Epoch [115/200], Loss: 0.5580\n",
      "Epoch [116/200], Loss: 0.5579\n",
      "Epoch [117/200], Loss: 0.5577\n",
      "Epoch [118/200], Loss: 0.5576\n",
      "Epoch [119/200], Loss: 0.5574\n",
      "Epoch [120/200], Loss: 0.5573\n",
      "Epoch [121/200], Loss: 0.5571\n",
      "Epoch [122/200], Loss: 0.5570\n",
      "Epoch [123/200], Loss: 0.5569\n",
      "Epoch [124/200], Loss: 0.5567\n",
      "Epoch [125/200], Loss: 0.5566\n",
      "Epoch [126/200], Loss: 0.5565\n",
      "Epoch [127/200], Loss: 0.5564\n",
      "Epoch [128/200], Loss: 0.5563\n",
      "Epoch [129/200], Loss: 0.5562\n",
      "Epoch [130/200], Loss: 0.5561\n",
      "Epoch [131/200], Loss: 0.5560\n",
      "Epoch [132/200], Loss: 0.5559\n",
      "Epoch [133/200], Loss: 0.5558\n",
      "Epoch [134/200], Loss: 0.5557\n",
      "Epoch [135/200], Loss: 0.5556\n",
      "Epoch [136/200], Loss: 0.5555\n",
      "Epoch [137/200], Loss: 0.5554\n",
      "Epoch [138/200], Loss: 0.5554\n",
      "Epoch [139/200], Loss: 0.5553\n",
      "Epoch [140/200], Loss: 0.5552\n",
      "Epoch [141/200], Loss: 0.5551\n",
      "Epoch [142/200], Loss: 0.5550\n",
      "Epoch [143/200], Loss: 0.5550\n",
      "Epoch [144/200], Loss: 0.5549\n",
      "Epoch [145/200], Loss: 0.5548\n",
      "Epoch [146/200], Loss: 0.5547\n",
      "Epoch [147/200], Loss: 0.5547\n",
      "Epoch [148/200], Loss: 0.5546\n",
      "Epoch [149/200], Loss: 0.5546\n",
      "Epoch [150/200], Loss: 0.5545\n",
      "Epoch [151/200], Loss: 0.5544\n",
      "Epoch [152/200], Loss: 0.5544\n",
      "Epoch [153/200], Loss: 0.5543\n",
      "Epoch [154/200], Loss: 0.5543\n",
      "Epoch [155/200], Loss: 0.5542\n",
      "Epoch [156/200], Loss: 0.5542\n",
      "Epoch [157/200], Loss: 0.5541\n",
      "Epoch [158/200], Loss: 0.5540\n",
      "Epoch [159/200], Loss: 0.5540\n",
      "Epoch [160/200], Loss: 0.5539\n",
      "Epoch [161/200], Loss: 0.5539\n",
      "Epoch [162/200], Loss: 0.5539\n",
      "Epoch [163/200], Loss: 0.5538\n",
      "Epoch [164/200], Loss: 0.5538\n",
      "Epoch [165/200], Loss: 0.5537\n",
      "Epoch [166/200], Loss: 0.5537\n",
      "Epoch [167/200], Loss: 0.5536\n",
      "Epoch [168/200], Loss: 0.5536\n",
      "Epoch [169/200], Loss: 0.5536\n",
      "Epoch [170/200], Loss: 0.5535\n",
      "Epoch [171/200], Loss: 0.5535\n",
      "Epoch [172/200], Loss: 0.5534\n",
      "Epoch [173/200], Loss: 0.5534\n",
      "Epoch [174/200], Loss: 0.5534\n",
      "Epoch [175/200], Loss: 0.5533\n",
      "Epoch [176/200], Loss: 0.5533\n",
      "Epoch [177/200], Loss: 0.5533\n",
      "Epoch [178/200], Loss: 0.5532\n",
      "Epoch [179/200], Loss: 0.5532\n",
      "Epoch [180/200], Loss: 0.5532\n",
      "Epoch [181/200], Loss: 0.5531\n",
      "Epoch [182/200], Loss: 0.5531\n",
      "Epoch [183/200], Loss: 0.5531\n",
      "Epoch [184/200], Loss: 0.5531\n",
      "Epoch [185/200], Loss: 0.5530\n",
      "Epoch [186/200], Loss: 0.5530\n",
      "Epoch [187/200], Loss: 0.5530\n",
      "Epoch [188/200], Loss: 0.5530\n",
      "Epoch [189/200], Loss: 0.5529\n",
      "Epoch [190/200], Loss: 0.5529\n",
      "Epoch [191/200], Loss: 0.5529\n",
      "Epoch [192/200], Loss: 0.5529\n",
      "Epoch [193/200], Loss: 0.5528\n",
      "Epoch [194/200], Loss: 0.5528\n",
      "Epoch [195/200], Loss: 0.5528\n",
      "Epoch [196/200], Loss: 0.5528\n",
      "Epoch [197/200], Loss: 0.5527\n",
      "Epoch [198/200], Loss: 0.5527\n",
      "Epoch [199/200], Loss: 0.5527\n",
      "Epoch [200/200], Loss: 0.5527\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "#model = NeuralNetwork2()\n",
    "model = NeuralNetwork()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    for x, y in dataset[0:300]:\n",
    "        # Forward pass\n",
    "        outputs = model(x)\n",
    "        #print(outputs)\n",
    "        #print(y)\n",
    "        loss = criterion(outputs, y)\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Print loss for monitoring training progress\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print('Training finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.2970e-01, 7.7005e-01, 2.4985e-04], grad_fn=<SoftmaxBackward0>)|tensor([1., 0., 0.])\n",
      "tensor([0.5958, 0.2392, 0.1651], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.5958, 0.2392, 0.1651], grad_fn=<SoftmaxBackward0>)|tensor([0., 0., 1.])\n",
      "tensor([0.0090, 0.6698, 0.3212], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.0090, 0.6698, 0.3212], grad_fn=<SoftmaxBackward0>)|tensor([0., 0., 1.])\n",
      "tensor([4.6771e-04, 9.9218e-01, 7.3520e-03], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([4.6771e-04, 9.9218e-01, 7.3520e-03], grad_fn=<SoftmaxBackward0>)|tensor([0., 0., 1.])\n",
      "tensor([0.4591, 0.5387, 0.0022], grad_fn=<SoftmaxBackward0>)|tensor([1., 0., 0.])\n",
      "tensor([9.2266e-01, 7.7324e-02, 1.6183e-05], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([9.2266e-01, 7.7324e-02, 1.6183e-05], grad_fn=<SoftmaxBackward0>)|tensor([0., 1., 0.])\n",
      "tensor([0.2256, 0.7735, 0.0009], grad_fn=<SoftmaxBackward0>)|tensor([1., 0., 0.])\n",
      "tensor([8.5947e-01, 1.4050e-01, 3.4697e-05], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([8.5947e-01, 1.4050e-01, 3.4697e-05], grad_fn=<SoftmaxBackward0>)|tensor([0., 1., 0.])\n",
      "tensor([0.3786, 0.0338, 0.5876], grad_fn=<SoftmaxBackward0>)|tensor([1., 0., 0.])\n",
      "tensor([0.0425, 0.8921, 0.0654], grad_fn=<SoftmaxBackward0>)|tensor([1., 0., 0.])\n",
      "tensor([0.2014, 0.1432, 0.6554], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.2014, 0.1432, 0.6554], grad_fn=<SoftmaxBackward0>)|tensor([0., 1., 0.])\n",
      "tensor([0.1878, 0.8101, 0.0021], grad_fn=<SoftmaxBackward0>)|tensor([1., 0., 0.])\n",
      "tensor([0.4767, 0.5122, 0.0111], grad_fn=<SoftmaxBackward0>)|tensor([1., 0., 0.])\n",
      "tensor([9.1639e-01, 8.2805e-02, 8.0407e-04], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([9.1639e-01, 8.2805e-02, 8.0407e-04], grad_fn=<SoftmaxBackward0>)|tensor([0., 1., 0.])\n",
      "tensor([9.0864e-01, 9.1307e-02, 5.2500e-05], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([9.0864e-01, 9.1307e-02, 5.2500e-05], grad_fn=<SoftmaxBackward0>)|tensor([0., 1., 0.])\n",
      "tensor([3.1163e-02, 9.6852e-01, 3.1336e-04], grad_fn=<SoftmaxBackward0>)|tensor([1., 0., 0.])\n",
      "tensor([5.2132e-05, 9.8611e-01, 1.3839e-02], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([5.2132e-05, 9.8611e-01, 1.3839e-02], grad_fn=<SoftmaxBackward0>)|tensor([0., 0., 1.])\n",
      "17\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Antti\\AppData\\Local\\Temp\\ipykernel_52420\\2662789135.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    }
   ],
   "source": [
    "#for x, y in dataset[300:]:\n",
    "    #print(x)\n",
    "    #print(y[1])\n",
    "    #output = model(x)\n",
    "    \n",
    "    #print(str(y) + \" | \" + str(output))\n",
    "\n",
    "\n",
    "\n",
    "#    if y[2].item() == 1.:\n",
    "#        print(y)\n",
    "#        print(output)\n",
    "\n",
    "\n",
    "#exit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "count = 0\n",
    "other_count = 0\n",
    "for x, y in dataset[300:]:\n",
    "    output = model(x)\n",
    "\n",
    "    #if output[0].item() > 0.55:\n",
    "    #    pred = 0\n",
    "    #elif output[2].item() > 0.25:\n",
    "    #    pred = 2\n",
    "    #else:\n",
    "    #    pred = 1\n",
    "    \n",
    "    #print(pred)\n",
    "    \n",
    "    pred = torch.argmax(output)\n",
    "\n",
    "    if y[pred] == 0.:\n",
    "        #print(str(output) + str(y))\n",
    "        count += 1\n",
    "        if(y[1] == 1 or y[2] == 1):\n",
    "            other_count += 1\n",
    "            print(output)\n",
    "        print(str(output) + \"|\" + str(y))\n",
    "\n",
    "\n",
    "    #if output.item() > 0.59 and y.item() == 0.:\n",
    "    #    count += 1\n",
    "    #elif output.item() <= 0.59 and y.item() == 1.:\n",
    "    #    count+= 1\n",
    "    #else:\n",
    "    #    print(str(output.item()) + \"|\" + str(y.item()))\n",
    "\n",
    "print(count)\n",
    "print(other_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_final.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (fc1): Linear(in_features=2, out_features=10, bias=True)\n",
       "  (fc2): Linear(in_features=10, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_model = NeuralNetwork()\n",
    "n_model.load_state_dict(torch.load('model2.pth'))\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "n_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error for GET to https://api.spotify.com/v1/me/player/currently-playing with Params: {} returned 401 due to The access token expired\n"
     ]
    },
    {
     "ename": "SpotifyException",
     "evalue": "http status: 401, code:-1 - https://api.spotify.com/v1/me/player/currently-playing:\n The access token expired, reason: None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Antti\\anaconda3\\envs\\cs4756_a5\\lib\\site-packages\\spotipy\\client.py:271\u001b[0m, in \u001b[0;36mSpotify._internal_call\u001b[1;34m(self, method, url, payload, params)\u001b[0m\n\u001b[0;32m    266\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    267\u001b[0m     method, url, headers\u001b[38;5;241m=\u001b[39mheaders, proxies\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxies,\n\u001b[0;32m    268\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequests_timeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs\n\u001b[0;32m    269\u001b[0m )\n\u001b[1;32m--> 271\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m results \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32mc:\\Users\\Antti\\anaconda3\\envs\\cs4756_a5\\lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://api.spotify.com/v1/me/player/currently-playing",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSpotifyException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m artist, name, _, features \u001b[38;5;241m=\u001b[39m \u001b[43mextract\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_current_song\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(name)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(features)\n",
      "File \u001b[1;32mc:\\Users\\Antti\\Documents\\Classes\\ECE5760\\Final_Proj\\extract.py:121\u001b[0m, in \u001b[0;36mget_current_song\u001b[1;34m()\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_current_song\u001b[39m():\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# Get track information\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m     track \u001b[38;5;241m=\u001b[39m \u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_user_playing_track\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m#pprint(track)\u001b[39;00m\n\u001b[0;32m    123\u001b[0m     artist \u001b[38;5;241m=\u001b[39m track[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitem\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124martists\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Antti\\anaconda3\\envs\\cs4756_a5\\lib\\site-packages\\spotipy\\client.py:1232\u001b[0m, in \u001b[0;36mSpotify.current_user_playing_track\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcurrent_user_playing_track\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Get information about the current users currently playing track.\u001b[39;00m\n\u001b[0;32m   1231\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mme/player/currently-playing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Antti\\anaconda3\\envs\\cs4756_a5\\lib\\site-packages\\spotipy\\client.py:323\u001b[0m, in \u001b[0;36mSpotify._get\u001b[1;34m(self, url, args, payload, **kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args:\n\u001b[0;32m    321\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(args)\n\u001b[1;32m--> 323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Antti\\anaconda3\\envs\\cs4756_a5\\lib\\site-packages\\spotipy\\client.py:293\u001b[0m, in \u001b[0;36mSpotify._internal_call\u001b[1;34m(self, method, url, payload, params)\u001b[0m\n\u001b[0;32m    286\u001b[0m         reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    288\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHTTP Error for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m with Params: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m returned \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m due to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    290\u001b[0m         method, url, args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m), response\u001b[38;5;241m.\u001b[39mstatus_code, msg\n\u001b[0;32m    291\u001b[0m     )\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SpotifyException(\n\u001b[0;32m    294\u001b[0m         response\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[0;32m    295\u001b[0m         \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (response\u001b[38;5;241m.\u001b[39murl, msg),\n\u001b[0;32m    297\u001b[0m         reason\u001b[38;5;241m=\u001b[39mreason,\n\u001b[0;32m    298\u001b[0m         headers\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    299\u001b[0m     )\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRetryError \u001b[38;5;28;01mas\u001b[39;00m retry_error:\n\u001b[0;32m    301\u001b[0m     request \u001b[38;5;241m=\u001b[39m retry_error\u001b[38;5;241m.\u001b[39mrequest\n",
      "\u001b[1;31mSpotifyException\u001b[0m: http status: 401, code:-1 - https://api.spotify.com/v1/me/player/currently-playing:\n The access token expired, reason: None"
     ]
    }
   ],
   "source": [
    "artist, name, _, features = extract.get_current_song()\n",
    "print(name)\n",
    "print(features)\n",
    "\n",
    "output = n_model(features)\n",
    "pred = torch.argmax(output)\n",
    "print(pred) # 0 = Neutral, 1, = \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1920e-04, 6.0804e-01, 1.5875e-04, 3.9168e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([0., 0., 1., 0.])\n",
      "tensor([8.5327e-02, 9.1268e-01, 1.0813e-04, 1.8866e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([0., 0., 1., 0.])\n",
      "tensor([3.0138e-06, 1.4027e-01, 5.6686e-05, 8.5967e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([0., 1., 0., 0.])\n",
      "tensor([7.7847e-03, 9.7227e-01, 1.5859e-04, 1.9787e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([0., 0., 1., 0.])\n",
      "tensor([1.3185e-01, 8.6692e-01, 9.6136e-05, 1.1348e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([0., 0., 1., 0.])\n",
      "tensor([1.8690e-03, 9.3012e-01, 1.7924e-04, 6.7830e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([0., 0., 0., 1.])\n",
      "tensor([9.6059e-02, 9.0197e-01, 1.1382e-04, 1.8542e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([1., 0., 0., 0.])\n",
      "tensor([2.1571e-02, 9.7783e-01, 7.9497e-05, 5.2248e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([0., 0., 1., 0.])\n",
      "tensor([1.4925e-03, 9.9273e-01, 1.0641e-04, 5.6697e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([0., 0., 0., 1.])\n",
      "tensor([4.5545e-01, 5.4451e-01, 2.7097e-05, 9.3862e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([1., 0., 0., 0.])\n",
      "tensor([2.3800e-01, 7.6143e-01, 7.5990e-05, 5.0132e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([0., 0., 1., 0.])\n",
      "tensor([1.0695e-01, 8.9290e-01, 5.6151e-05, 9.1319e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([1., 0., 0., 0.])\n",
      "tensor([2.1434e-03, 9.9305e-01, 1.0962e-04, 4.6939e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([0., 0., 1., 0.])\n",
      "tensor([9.9246e-01, 7.5357e-03, 3.6496e-07, 1.9627e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([0., 1., 0., 0.])\n",
      "tensor([1.8857e-03, 9.3057e-01, 1.7929e-04, 6.7369e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([0., 0., 1., 0.])\n",
      "tensor([1.0710e-02, 9.7414e-01, 1.5413e-04, 1.4993e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([0., 0., 1., 0.])\n",
      "tensor([1.5565e-02, 9.7261e-01, 1.5706e-04, 1.1665e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([0., 0., 0., 1.])\n",
      "tensor([1.6767e-03, 9.1538e-01, 1.9634e-04, 8.2746e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([0., 0., 0., 1.])\n",
      "tensor([9.1505e-01, 8.4950e-02, 2.8182e-06, 1.1646e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([0., 1., 0., 0.])\n",
      "tensor([2.8629e-05, 3.6656e-01, 1.1971e-04, 6.3329e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([0., 1., 0., 0.])\n",
      "tensor([3.3858e-04, 7.6621e-01, 1.8326e-04, 2.3326e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([0., 0., 0., 1.])\n",
      "tensor([5.9308e-01, 4.0682e-01, 3.3549e-05, 6.4797e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([0., 0., 1., 0.])\n",
      "tensor([9.3050e-02, 9.0680e-01, 5.5178e-05, 9.6644e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([0., 0., 1., 0.])\n",
      "tensor([9.6014e-01, 3.9860e-02, 1.2814e-06, 2.9698e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([0., 1., 0., 0.])\n",
      "tensor([3.6297e-03, 9.5677e-01, 1.7323e-04, 3.9425e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([0., 0., 0., 1.])\n",
      "tensor([8.2246e-04, 8.6899e-01, 1.8814e-04, 1.3000e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([0., 0., 0., 1.])\n",
      "tensor([8.3198e-01, 1.6801e-01, 7.0288e-06, 6.1465e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([0., 1., 0., 0.])\n",
      "tensor([9.6736e-01, 3.2639e-02, 1.9912e-06, 3.5920e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([0., 1., 0., 0.])\n",
      "tensor([7.0556e-04, 9.8857e-01, 1.1289e-04, 1.0613e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([0., 0., 0., 1.])\n",
      "tensor([9.8016e-01, 1.9836e-02, 1.0546e-06, 1.2242e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([0., 0., 1., 0.])\n",
      "tensor([3.9342e-01, 6.0631e-01, 5.8143e-05, 2.1907e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([1., 0., 0., 0.])\n",
      "tensor([9.9327e-01, 6.7272e-03, 3.0070e-07, 1.4179e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([0., 0., 1., 0.])\n",
      "tensor([1.2789e-05, 6.6162e-01, 1.5529e-04, 3.3821e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([0., 0., 0., 1.])\n",
      "tensor([3.1211e-02, 9.6843e-01, 6.7464e-05, 2.9579e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([0., 0., 1., 0.])\n",
      "tensor([7.6079e-01, 2.3917e-01, 1.8413e-05, 1.9324e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([0., 1., 0., 0.])\n",
      "tensor([2.3252e-04, 7.0821e-01, 1.7922e-04, 2.9138e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)|tensor([0., 0., 0., 1.])\n",
      "36\n",
      "22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Antti\\AppData\\Local\\Temp\\ipykernel_42348\\1103024500.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "other_count = 0\n",
    "for x, y in dataset[300:]:\n",
    "    output = n_model(x)\n",
    "\n",
    "    #if output[0].item() > 0.55:\n",
    "    #    pred = 0\n",
    "    #elif output[2].item() > 0.25:\n",
    "    #    pred = 2\n",
    "    #else:\n",
    "    #    pred = 1\n",
    "    \n",
    "    #print(pred)\n",
    "    \n",
    "    pred = torch.argmax(output)\n",
    "\n",
    "    if y[pred] == 0.:\n",
    "        #print(str(output) + str(y))\n",
    "        count += 1\n",
    "        if(y[1] == 1 or y[2] == 1):\n",
    "            other_count += 1\n",
    "        print(str(output) + \"|\" + str(y))\n",
    "\n",
    "\n",
    "    #if output.item() > 0.59 and y.item() == 0.:\n",
    "    #    count += 1\n",
    "    #elif output.item() <= 0.59 and y.item() == 1.:\n",
    "    #    count+= 1\n",
    "    #else:\n",
    "    #    print(str(output.item()) + \"|\" + str(y.item()))\n",
    "\n",
    "print(count)\n",
    "print(other_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
